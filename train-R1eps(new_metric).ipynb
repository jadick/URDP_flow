{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.utils.spectral_norm as spectralnorm\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 23 11:08:49 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P8    15W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_models_state(list_models, state):\n",
    "    if state =='train':\n",
    "        for model in list_models:\n",
    "            model.train()\n",
    "    else:\n",
    "        for model in list_models:\n",
    "            model.eval()\n",
    "\n",
    "def set_opt_zero(opts):\n",
    "    for opt in opts:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0),  1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake[:,0],\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def cal_W1(ssf, encoder, decoder, decoder_hat, discriminator, discriminator_M, test_loader, list_models):\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    mse_avg = nn.MSELoss()\n",
    "    set_models_state(list_models, 'eval')\n",
    "\n",
    "    W1_distance = []\n",
    "    W1M_distance = []\n",
    "    MSE = []\n",
    "\n",
    "    num_x = 0\n",
    "    for i, x in enumerate(iter(test_loader)):\n",
    "        with torch.no_grad():\n",
    "            #Get the data\n",
    "            x = x.permute(0, 4, 1, 2, 3)\n",
    "            x = x.cuda().float()\n",
    "            x_cur = x[:,:,1,...]\n",
    "            with torch.no_grad():\n",
    "                hx = encoder(x[:,:,0,...])[0]\n",
    "                x_ref = decoder(hx).detach()\n",
    "                x_1_hat = decoder_hat(hx).detach()\n",
    "            #x_ref[x_ref < 0.1] = 0.0\n",
    "            x_hat = ssf(x_cur, x_ref, x_1_hat)\n",
    "\n",
    "\n",
    "            fake_vid = torch.cat((x_1_hat, x_hat), dim = 1).detach()\n",
    "            real_vid = x[:,0,:2,...].detach() #this looks good!\n",
    "\n",
    "            fake_validity = discriminator(fake_vid)\n",
    "            real_validity = discriminator(real_vid)\n",
    "\n",
    "            fake_img = x_hat.detach()\n",
    "            real_img = x[:,0,6:7,...].detach()\n",
    "            fake_valid_m = discriminator_M(fake_img)\n",
    "            real_valid_m = discriminator_M(real_img)\n",
    "\n",
    "            W1_distance.append(torch.sum(real_validity) - torch.sum(fake_validity))\n",
    "            W1M_distance.append(torch.sum(real_valid_m) - torch.sum(fake_valid_m))\n",
    "            #print (F.mse_loss(x[:,:,1,:,:], x_hat)* x.size()[0])\n",
    "            MSE.append(mse_loss(x[:,:,1,:,:], x_hat))\n",
    "            num_x += len(x)\n",
    "\n",
    "    W1_distance = torch.Tensor(W1_distance)\n",
    "    W1M_distance = torch.Tensor(W1M_distance)\n",
    "    MSE = torch.Tensor(MSE)\n",
    "\n",
    "    return W1M_distance.sum()/num_x, W1_distance.sum()/num_x, MSE.sum()/(64*64*num_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified test function for new metric\n",
    "def cal_W1_new(ssf, encoder, decoder, decoder_hat, discriminator, discriminator_N, test_loader, list_models):\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    mse_avg = nn.MSELoss()\n",
    "    set_models_state(list_models, 'eval')\n",
    "\n",
    "    W1_distance = []\n",
    "    W1N_distance = []#N stands for 'New'\n",
    "    MSE = []\n",
    "\n",
    "    num_x = 0\n",
    "    for i, x in enumerate(iter(test_loader)):\n",
    "        with torch.no_grad():\n",
    "            #Get the data\n",
    "            x = x.permute(0, 4, 1, 2, 3)\n",
    "            x = x.cuda().float()\n",
    "            x_cur = x[:,:,1,...] # x_2\n",
    "            with torch.no_grad():\n",
    "                hx = encoder(x[:,:,0,...])[0]\n",
    "                x_ref = decoder(hx).detach()\n",
    "                x_1_hat = decoder_hat(hx).detach()\n",
    "            x_2_hat = ssf(x_cur, x_ref, x_1_hat) # x_hat is x2_hat\n",
    "\n",
    "            fake_vid = torch.cat((x_1_hat, x_2_hat), dim = 1).detach()\n",
    "            real_vid = x[:,0,:2,...].detach()\n",
    "            fake_validity = discriminator(fake_vid)\n",
    "            real_validity = discriminator(real_vid)\n",
    "            \n",
    "            new_metric_real_vid = torch.cat((x_1_hat, x_cur), dim = 1).detach()\n",
    "            new_metric_real_validity = discriminator_N(new_metric_real_vid) # # substitute real video by new_metric_vid\n",
    "            new_metric_fake_validity = discriminator_N(fake_vid) # substitute new_metric_vid by fake_vid\n",
    "\n",
    "            W1N_distance.append(torch.sum(new_metric_real_validity) - torch.sum(new_metric_fake_validity))\n",
    "            W1_distance.append(torch.sum(real_validity) - torch.sum(fake_validity))\n",
    "            MSE.append(mse_loss(x[:,:,1,:,:], x_2_hat))\n",
    "            num_x += len(x)\n",
    "            #break\n",
    "\n",
    "    W1_distance = torch.Tensor(W1_distance)\n",
    "    MSE = torch.Tensor(MSE)\n",
    "    W1N_distance = torch.Tensor(W1N_distance)\n",
    "\n",
    "    return (W1_distance.sum()/num_x).item(), (MSE.sum()/(64*64*num_x)).item(), (W1N_distance.sum()/num_x).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, 1.0]\n",
      "[-1.0, 1.0]\n",
      "Finished Loading MNIST!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-90644.8828125, 0.2323826551437378, -82575.921875)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code to test call_W1_new()\n",
    "\n",
    "discriminator = Discriminator_v3(out_ch=2).to(device) #Generator Side\n",
    "discriminator_N = Discriminator_v3(out_ch=2).to(device) #Generator Side\n",
    "ssf = ScaleSpaceFlow_R1eps(num_levels=1, dim=8, stochastic=True, quantize_latents=True, L=2).to(device)\n",
    "\n",
    "list_models = [discriminator, discriminator_N, ssf]\n",
    "\n",
    "encoder = Encoder(dim=12, nc=1, stochastic=True, quantize_latents=True, L=2).to(device) #Generator Side\n",
    "decoder = Decoder_Iframe(dim=12).to(device) #Generator Side\n",
    "decoder_hat = Decoder_Iframe(dim=12).to(device)\n",
    "\n",
    "train_loader, test_loader = get_dataloader(data_root='./data/', seq_len=8, batch_size=64, num_digits=1)\n",
    "\n",
    "cal_W1_new(ssf, encoder, decoder, decoder_hat, discriminator, discriminator_N, test_loader, list_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_new(dim = 128,\n",
    "        z_dim = 12,\n",
    "        lambda_gp = 10,\n",
    "        bs = 64,\n",
    "        d_penalty = 0,\n",
    "        skip_fq = 1,\n",
    "        total_epochs = 3,\n",
    "        lambda_P = 0,\n",
    "        lambda_PN = 0,\n",
    "        lambda_MSE = 1,\n",
    "        L = 2,\n",
    "        path = './data/',\n",
    "        pre_path = 'None'):\n",
    "        \n",
    "    #No quantization:\n",
    "    stochastic = True\n",
    "    quantize_latents = True\n",
    "    if L == -1:\n",
    "        stochastic = False\n",
    "        quantize_latents = False\n",
    "    #Create folder:\n",
    "    folder_name='R1-eps|dim_'+str(dim)+'|z_dim_'+str(z_dim)+'|L_'+str(L)+'|lambda_gp_'+str(lambda_gp) \\\n",
    "        +'|bs_'+str(bs)+'|dpenalty_'+str(d_penalty)+'|lambdaP_'+str(lambda_P)+'|lambdaPM_'+str(lambda_PN)+'|lambdaMSE_' + str(lambda_MSE)\n",
    "    print (\"Settings: \", folder_name)\n",
    "\n",
    "    os.makedirs('./saved_models/'+ folder_name, exist_ok=True)\n",
    "    f = open('./saved_models/'+ folder_name + \"/performance.txt\", \"a\")\n",
    "\n",
    "    #Define Models\n",
    "    discriminator = Discriminator_v3(out_ch=2).to(device) #Generator Side\n",
    "    discriminator_N = Discriminator_v3(out_ch=2).to(device) #Marginal Discriminator\n",
    "    ssf = ScaleSpaceFlow_R1eps(num_levels=1, dim=z_dim, stochastic=stochastic, quantize_latents=quantize_latents, L=L).to(device)\n",
    "    list_models = [discriminator, discriminator_N, ssf]\n",
    "\n",
    "    #Load models:\n",
    "    if pre_path != 'None':\n",
    "        ssf.motion_encoder.load_state_dict(torch.load(pre_path+'/m_enc.pth'))\n",
    "        ssf.motion_decoder.load_state_dict(torch.load(pre_path+'/m_dec.pth'))\n",
    "        ssf.P_encoder.load_state_dict(torch.load(pre_path+'/p_enc.pth'))\n",
    "        ssf.res_encoder.load_state_dict(torch.load(pre_path+'/r_enc.pth'))\n",
    "        ssf.res_decoder.load_state_dict(torch.load(pre_path+'/r_dec.pth'))\n",
    "        discriminator.load_state_dict(torch.load(pre_path+'/discriminator.pth'))\n",
    "        discriminator_M.load_state_dict(torch.load(pre_path+'/discriminator_N.pth'))\n",
    "    \n",
    "    #Define fixed models\n",
    "    I_dim = 12 \n",
    "    I_L = 2\n",
    "    encoder = Encoder(dim=I_dim, nc=1, stochastic=True, quantize_latents=True, L=I_L).to(device).eval() #Generator Side\n",
    "    decoder = Decoder_Iframe(dim=I_dim).to(device).eval()\n",
    "    decoder_hat = Decoder_Iframe(dim=I_dim).to(device).eval()\n",
    "    encoder.load_state_dict(torch.load('./I3/I_frame_encoder_zdim_12_L_2.pth'))\n",
    "    decoder.load_state_dict(torch.load('./I3/I_frame_decoderMMSE_zdim_12_L_2.pth'))\n",
    "    decoder_hat.load_state_dict(torch.load('./I3/I_frame_decoder_zdim_12_L_2.pth'))\n",
    "\n",
    "    #Define Data Loader\n",
    "    train_loader, test_loader = get_dataloader(data_root=path, seq_len=8, batch_size=bs, num_digits=1)\n",
    "    loader_l = len(train_loader)\n",
    "    mse = torch.nn.MSELoss()\n",
    "\n",
    "    #Define optimizers\n",
    "    opt_ssf= torch.optim.RMSprop(ssf.parameters(), lr=1e-5)\n",
    "    opt_d = torch.optim.RMSprop(discriminator.parameters(), lr=5e-5)\n",
    "    opt_dn = torch.optim.RMSprop(discriminator_N.parameters(), lr=5e-5)\n",
    "    list_opt = [opt_ssf, opt_d, opt_dn]\n",
    "\n",
    "    \n",
    "    for epoch in range(total_epochs):\n",
    "        set_models_state(list_models, 'train')\n",
    "        a1 = time.time()\n",
    "        for i,x in enumerate(train_loader):\n",
    "            if i%100 ==0:\n",
    "                print(f'{i}/{loader_l}')\n",
    "            set_opt_zero(list_opt)\n",
    "            \n",
    "            #Get the data\n",
    "            x = x.permute(0, 4, 1, 2, 3)\n",
    "            x = x.cuda().float()\n",
    "            x_cur = x[:,:,1,...]\n",
    "            with torch.no_grad():\n",
    "                hx = encoder(x[:,:,0,...])[0]\n",
    "                x_ref = decoder(hx).detach()\n",
    "                x_1_hat = decoder_hat(hx).detach()\n",
    "                x_hat = ssf(x_cur, x_ref, x_1_hat)\n",
    "\n",
    "\n",
    "            \n",
    "            #Optimize discriminator\n",
    "            fake_vid = torch.cat((x_1_hat, x_hat), dim = 1)\n",
    "            real_vid = x[:,0,:2,...].detach() #this looks good!\n",
    "            fake_validity = discriminator(fake_vid.detach())\n",
    "            real_validity = discriminator(real_vid)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_vid.data, fake_vid.data)\n",
    "            errVD =  -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "            errVD.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "            #Optimize discriminator_N\n",
    "            \n",
    "            new_metric_real_vid = torch.cat((x_1_hat, x_cur), dim = 1).detach()\n",
    "            new_metric_real_validity = discriminator_N(new_metric_real_vid)\n",
    "            new_metric_fake_validity = discriminator_N(fake_vid)\n",
    "            gradient_penalty_new = compute_gradient_penalty(discriminator_N, new_metric_real_vid.data, fake_vid.data)\n",
    "            errVDN =  -torch.mean(new_metric_real_validity) + torch.mean(new_metric_fake_validity) + lambda_gp * gradient_penalty_new\n",
    "            errVDN.backward()\n",
    "            opt_dn.step()         \n",
    "            \n",
    "            \n",
    "            if i%skip_fq == 0:\n",
    "                x_cur = x_cur.detach()\n",
    "                x_ref = x_ref.detach()\n",
    "                x_1_hat = x_1_hat.detach()\n",
    "                x_hat = ssf(x_cur, x_ref, x_1_hat)\n",
    "                \n",
    "                #discriminator\n",
    "                fake_vid = torch.cat((x_1_hat, x_hat), dim = 1)\n",
    "                fake_validity = discriminator(fake_vid)\n",
    "                errVG = -torch.mean(fake_validity)\n",
    "                \n",
    "                #discriminator_N\n",
    "                new_metric_real_vid = torch.cat((x_1_hat, x_cur), dim = 1).detach()\n",
    "                new_metric_fake_validity = discriminator_N(fake_vid)\n",
    "                errVGN = -torch.mean(new_metric_fake_validity)\n",
    "                \n",
    "                loss = lambda_MSE*mse(x_hat, x_cur)  #lambda_P*errVG + lambda_PN*errVGN\n",
    "                print(loss.item())\n",
    "                loss.backward()\n",
    "                opt_ssf.step()\n",
    "                \n",
    "        if epoch  >= 0:\n",
    "            evaluation = cal_W1_new(ssf, encoder, decoder, decoder_hat, discriminator, discriminator_N, test_loader, list_models)\n",
    "            show_str = f\"Epoch: {epoch} | l_PN, l_P, l_MSE, d_penalty {lambda_PN} {lambda_P} {lambda_MSE} {d_penalty} | Eval loss (joint,mse,new): {evaluation})\"\n",
    "            print (show_str)\n",
    "            f.write(show_str+\"\\n\")\n",
    "    set_models_state(list_models, 'eval')\n",
    "\n",
    "    torch.save(ssf.motion_encoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'm_enc.pth'))\n",
    "    torch.save(ssf.motion_decoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'm_dec.pth'))\n",
    "    torch.save(ssf.P_encoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'p_enc.pth'))\n",
    "    torch.save(ssf.res_encoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'r_enc.pth'))\n",
    "    torch.save(ssf.res_decoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'r_dec.pth' ))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'discriminator.pth'))\n",
    "    torch.save(discriminator_N.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'discriminator_N.pth'))\n",
    "    f.close()\n",
    "\n",
    "    #save some figures\n",
    "    for i,x in enumerate(iter(train_loader)):\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        x = x.cuda().float()\n",
    "        break\n",
    "    np.savez_compressed(\"./saved_models/\" + folder_name+\"/x\", a=x.detach().cpu().numpy())\n",
    "\n",
    "    for i in range(5): #generate same figure 5 times\n",
    "        x_cur = x[:,:,1,...]\n",
    "        x_ref = x[:,:,0,...]\n",
    "        x_hat = ssf(x_cur, x_ref)\n",
    "        np.savez_compressed(\"./saved_models/\" + folder_name+\"/x_hat\"+str(i), a=x_hat.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 8, 64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABMCAYAAABwIzxgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAReklEQVR4nO3df0xU55oH8O8zP2F0sQgKCAolqNgfaltQLqBhm7Ipt9rbP26ppO1SSjUajd1Es7Rds3abmpTEu9dsYje28Vqr3Vatva61ibZRqpGYq3YRW9sZAZ2pOPzwB+IsODgwz/7BmVlQoCDDOZyZ55O8Gc6Zg+eZZ3wf3vOeOXOImSGEEEK/DFoHIIQQYnSkkAshhM5JIRdCCJ2TQi6EEDonhVwIIXROCrkQQujcqAo5ET1LRA4iqieit0IVlBBCiOGjB/0cOREZAVwEUAigEcAZACXM/HPowhNCCPFbRjMiXwCgnpkvMfNdAF8A+ENowhJCCDFcplH8bjKAK32WGwEsvHcjIloBYIWy+NQo9ieEEJHqOjNPGezJ0RRyGmDdffM0zPwRgI8AgIjk+wCEEGLkXEM9OZqplUYA0/sspwBwj+LfE0KIsFBYWIivvvoKS5YsgdlsHvP9jaaQnwEwk4geJiILgGUADoYmLCGGLyYmBmvWrIHT6URxcbHW4QypsLAQhw8fxpIlS7QO5T5r1qyB2+1GcXGxKsUnnD399NMoKirCtWvX4Pf7x3x/Dzy1wszdRLQGwBEARgB/YeYLIYtMiGGKiYlBcnIyurq64HINeQSqqZiYGMyePRuZmZmw2Wxah3Of5ORkxMbGwuVyqVJ8wpXRaER6ejqYGS6XC6p8wywzq9bQO4c+pi0mJobdbjcXFxez2Wwe8/1FUissLOTDhw/zkiVLNI+lb1u3bh3b7Xb++OOPOSoqSvN4Bmt5eXm8Z88erqqq4sTERM3j6dsmTZrEdrudW1paOCoqipXzWeOm6alfz549m/1+P1+6dCmU/+7ZoWpr2F3ZGRMTo6tRhdpzaQ9qPI8ms7OzMWvWLDidTni9Xq3DGVROTg7mzZuH+vp63Lp1S+tw+klMTMSsWbPQ0dEBr9erzihyBPTSr4kIiYmJYGY0NTWptt+wKuSTJk3CSy+9hNu3b6O2tnZcv+EBas+lPajHH38cixYtwuXLl3HixAmtwwmy2WzIy8vDnTt3sHPnTq3DGRQR4ZlnnsHMmTOxc+fOcfUHx2azoaysDHfu3MHZs2e1Duc+eurXCxcuxMaNG3HhwgXs3r1btf2GVSFPTExEdnb2uB1V3EuTubQHNF5Hk3PnzkVCQgIaGhrQ0tKidTiDio6OxmOPPQav1wun06l1OP3MnTsXCxYsQENDA6qqqrQO5z566tfZ2dlITk6Gy+VCTU2NavsNq0JeVlaGvLy8cTmqGEhGRgZefPFFNDc3o7m5edyONMbzaLKoqAgmkwl2ux0+n0/rcAZVXl6O5ORkHDt2DI2NjVqH009RURHS09Nht9vHZSHXS78mIuTk5GDKlCloaGjA5cuX79vGZDLBYrHAYAhx6Q2nk50Oh4P9fj/n5ORofsLjt5rNZuMPPviAOzo6eO/evZrHM1TLycnhu3fv8vnz58fViSaDwcA//vgj19XV8erVq4PrjUYjW61Wnj9/Pi9dupSLi4s5NzeXTSaTZu/1Z599xrdu3eItW7YM+DpMJhObzWY2GAya5JCZ++UQAFutVo6Pj9c8h3rp10ajMRjryy+/HFxvsVh48eLFXF5ezk1NTez3+7mqqopLS0vZaDQO998f8mTnaK7sHHesVisAjKtD/8GM98PZvrKzs2EymeByucbVqJeIYLFY0NbW1u89z8jIQEJCAlatWoWCggLExcWhuroar776KpqamtDT06NqnA899BDmzp2La9euoa6uLrjeaDTCZDJhzpw5mD59OqKjo9HY2IjTp0+ju7tbldgCOWTmfjm0WCxYuHAhZs6ciffff1/THOqlX1ssFsTGxgJAcFolKioKGRkZOHDgAGw2G3p6etDR0YH8/HzMmjULR44cQXNz86j3HRZTKwaDAdOmTUNqaioaGhpgt9uDz1mtVsTHx2Pp0qVYtmwZcnNzMWXKoF9ZoJrxfjgbYLPZkJOTg9u3b6OhoeG+5w0GAywWiyafuAl0HIfDAYfDgRkzZqC8vBy1tbXYt28fzp07h61bt2LNmjWYPHkyTp48iddff131ON98800kJCTg2LFjqKqqgsFgQFRUFPbu3YumpiZUV1fjiy++wPbt23H8+HEsX74cRAN9A0bo9S0+DocDZrMZ5eXlqKurw759+5Cfn69ZDvXWr/1+f/AP8OLFi5Gbm4uLFy+ipqYG69atw/z58/Hwww8jNTUVZ8+eRWxsbMguYAuLETkRYdq0aWBmtLW1BdffO6pISEjA8ePH8cknn2D37t2qj8z6xpuRkYGYmBi43e5+MQeYTCYYDAZ0d3drOnc+1GgyNjYWKSkpyMzMhN/vR1VVFdra2lQbTQY6zrx585CVlYXFixcjPz8fR44cwYEDB3Do0CEQEWJjY1FWVoaUlBRER0erEttAbDYbbDYbUlJSUFhYiOeeew7t7e2orKyE2WxGa2srVq9ejYqKCuzYsUOVcxF9i09WVhYsFgs2btyIpKQkrFixAqdOncLNmzc1yaHe+nVPTw88Hg8SExORm5uLrKwsEBH+9Kc/4ZtvvkFbWxv8fj9SUlKQnp4Or9eL6urq0Ow8HObIo6OjubKykv1+P+/atYvNZjOXl5ezy+XilpYW3rFjB2/YsIFLS0u5traWOzs7efny5ZpdPDLGc2khbZWVldza2srbtm3jRx55hA0GA0dFRfH+/fvZ6/VyR0cHezwe9ng8fPXqVV61apVqF5ME5nevX7/OdrudL168yBUVFWy1WoN5TklJ4UuXLnF3dzc///zzPGHCBNVz+OSTT7LD4eD6+nr+9NNPuaWlha9cucL5+fk8derUYKybNm3ilpYW/vDDD1V7vwM59Pv9bLfbua6ujisqKjgjIyO4jVY51Fu/JiL++uuv2ePxsNfrZY/Hw2+99RaXlZXxa6+9xqWlpfzuu+/y999/zx6Phzdv3jySfz/858j9fj9aW1sBAPPmzUN2dvaAo4ru7m6sXLkSQO/HwUJ+5niYtJxLe1ADjSY///xz2O12NDc3w2q1YsuWLaqOJpkZTqcTmZmZmDhxIhoaGkBEWLZsGYgIJpMJKSkpSE1NRWdnJ37++Wd0dHSMeVz3unr1Ku7evYukpCQsWLAA7e3t2L59O86cOYOuri4YjUYkJSWhpKQEcXFxOHz4sGqjykAO09LSkJaWBp/PByLCokWLkJ+fD2YOTgeonUO99WtmxsmTJ5GWloZHH30URqMRa9euBdB7dMHMsFqtMJlMOHDgAL788svQ7lzvI3KDwcAJCQns9/v5+vXrg44q3njjDe7u7uaamhpNRmaBZrVa2e12s9/v55UrV3Jubi7/+uuv7PP5uKysjDMzM3nq1Kk8efJkPnXqFHd2dvLatWs1iXWo0WTgEyypqam8adMmvnnzpqqjSaD3svcLFy6wx+Nhv9/fr7lcLq6treWSkhKOi4vT7P02Go1cUVHBXV1d7Pf72ePxsNvtZrfbzU1NTdzU1MQ3btzgc+fO8a5du9hms6kaX15eHm/dunXAHLa3t2uWQ73160CLj4/no0eP8s2bN4Pt2rVrvH//fp49e3bwiHGELfxH5MwMr9eLjo4OTJo0CVardcBRRUFBAe7cuYOjR49qMjIL0HQubYSGGk36fD4YjUYUFhaipKQELpdL1dEkAFRXV2Pz5s145ZVX8MQTTwTX9/T0YMOGDTh9+jQcDodq8Qykp6cHJ0+eRH19PWbMmIEJEyZgwoQJweevXLmC1tZWVFZW4ttvv0VnZ6eq8VVXV8PhcCAzM/O+HJ44cQLvvPOOJjnUW78OuHHjBl544QUYjcZ+630+H7xe75j0jwe+Z+cD7WyMbyyxdetWFBQUYM6cOf3WezweOJ1O1NfXY8WKFcFCqRUiwsGDB1FQUACz2Qyfz4dNmzahpaUl+Bc28B/0qaeewrZt27B+/XpNYjUajVi/fj3ee+89mM1mdHR0wOPx9HstNpsNly9fxsqVK3H+/HnVC5FeWK1WWCyW+zp4Z2cnenp6NDtJN97ppV+PsR+YOWvQZ8NhaiXQ4uPjuaysLNSHNGPSKioqgieZfD5fv0Ntt9vNN27c4Pb2dt61a5fmF0IMNn3R3t7OLpeL9+/fzyUlJapfzCItMpqe+vUYtiGnVsJqRK438fHx2LNnz6CHs06nE11dXRpG+P8GGk36fD74fD7cvXtXw8iEiAhDjsilkGuIiDBx4kRV59KEELo0ZCEPi5OdesXM/eabhRDiQYTFJfpCCBHJpJALIYTOSSEXQgidk0IuhBA6J4VcCCF0Tgq5EELonBRyIYTQuWF9jpyInAA8AHoAdDNzFhFNBrAHQBoAJ4BiZr7/DglCCCHG1EhG5H/PzPP7XF30FoCjzDwTwFFlWQghhMpGM7XyBwA7lZ93Anhh9OEIIYQYqeEWcgbwLRH9QEQrlHUJzNwEAMrj1IF+kYhWENFZIjo7+nCFEELca7jftZLHzG4imgrgOyKy/+ZvKJj5IwAfAfKlWUIIMRaGNSJnZrfy2ArgrwAWAGghoiQAUB5bxypIIYQQg/vNQk5EE4jo7wI/A/gHAD8BOAigVNmsFMB/j1WQQgghBjecqZUEAH8losD2/8XMh4noDIC9RFQO4FcAL45dmEIIIQYjN5YQQojxb8gbS8iVnUIIoXNSyIUQQuekkAshhM6pfc/O/wXgUHmf41E8gOtaB6ExyYHkIEDy8Ns5SB3ql9Uu5I6hJuwjBRGdjfQ8SA4kBwGSh9HnQKZWhBBC56SQCyGEzqldyD9SeX/jleRBcgBIDgIkD6PMgaoXBAkhhAg9mVoRQgidk0IuhBA6p1ohJ6JnichBRPVEFLa3hSOivxBRKxH91GfdZCL6jojqlMdYZT0R0X8oOTlPRE9qF3noENF0Iqoiol+I6AIRvamsj7Q8RBHRaSKqVfLwb8r6h4nob0oe9hCRRVlvVZbrlefTtIw/lIjISEQ1RHRIWY6oHBCRk4h+JKJzgZvshLI/qFLIicgIYCuAIgCPACghokfU2LcGPgHw7D3rBru/aRGAmUpbAeA/VYpxrHUDWMfMcwDkAFitvN+RlocuAE8z8zwA8wE8S0Q5ACoB/FnJQxuAcmX7cgBtzJwB4M/KduHiTQC/9FmOxBwM977HI+8PzDzmDcDvABzps/w2gLfV2LcWDUAagJ/6LDsAJCk/J6H3wigA2AagZKDtwqmh97vqCyM5DwBsAP4HwEL0XsFnUtYH+waAIwB+p/xsUrYjrWMPwWtPUQrV0wAOAaAIzIETQPw960LWH9SaWkkGcKXPcqOyLlIMdn/TsM+Lcmj8BIC/IQLzoEwpnEPvHbS+A9AA4BYzdyub9H2twTwoz7cDiFM34jGxBcA/A/Ary3GIvByM5L7HI+4Pal2iTwOsk889hnleiGgigP0A/omZbys3Jxlw0wHWhUUemLkHwHwiegi9t0mcM9BmymPY5YGIlgBoZeYfiKggsHqATcM2B4qR3Pd4xDlQa0TeCGB6n+UUAG6V9j0eDHZ/07DNCxGZ0VvEP2Pmr5TVEZeHAGa+BeB79J4zeIiIAoOovq81mAfl+UkAbqobacjlAXieiJwAvkDv9MoWRFYOwCO77/GI+4NahfwMgJnKmWoLgGXovednpBjs/qYHAfyjcpY6B0B74FBLz6h36L0dwC/M/O99noq0PExRRuIgomgAz6D3hF8VgD8qm92bh0B+/gjgGCuTpHrFzG8zcwozp6G33x9j5pcRQTmgkd/3eOT9QcXJ/t8DuIjeOcJ/0frkwxi+zs8BNAHwofcvazl65/iOAqhTHicr2xJ6P83TAOBHAFlaxx+iHOSj91DwPIBzSvt9BOZhLoAaJQ8/AfhXZX06gNMA6gHsA2BV1kcpy/XK8+lav4YQ56MAwKFIy4HyWmuVdiFQ/0LZH+QSfSGE0Dm5slMIIXROCrkQQuicFHIhhNA5KeRCCKFzUsiFEELnpJALIYTOSSEXQgid+z/5EtxpIA4qEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('       x_current     |      x_reference')\\nplt.imshow(np.hstack([x_curr,x_ref]), cmap='gray')\\nplt.show()\\n    \\nprint('       x_current     |      x_reference')\\nx_hat0 = np.load(f'{path}x_hat0.npz')['a'][0,0,:,:]\\nx_hat1 = np.load(f'{path}x_hat1.npz')['a'][0,0,:,:]\\nx_hat2 = np.load(f'{path}x_hat2.npz')['a'][0,0,:,:]\\nx_hat3 = np.load(f'{path}x_hat3.npz')['a'][0,0,:,:]\\nx_hat4 = np.load(f'{path}x_hat4.npz')['a'][0,0,:,:]\\n\\nplt.imshow(np.hstack([x_hat0,x_hat1,x_hat2,x_hat3,x_hat4]), cmap='gray')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lambda_PN = []\n",
    "path = './saved_models/R1-eps|_dim_128|z_dim_12|L_2|lambda_gp_10|bs_64|dpenalty_0.0|lambdaP_0.0|lambdaPN_0.0|lambdaMSE_1.0/'\n",
    "\n",
    "\n",
    "x_curr = np.load(f'{path}x.npz')['a'][0,0,1,...]\n",
    "x_ref = np.load(f'{path}x.npz')['a'][0,0,0,...]\n",
    "x = np.load(f'{path}x.npz')['a']\n",
    "print(x.shape)\n",
    "\n",
    "for v in range(5):\n",
    "    video = []\n",
    "    for i in range(8):\n",
    "        video.append(x[v,0,i,:,:])\n",
    "    plt.imshow(np.hstack(video), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "print('       x_current     |      x_reference')\n",
    "plt.imshow(np.hstack([x_curr,x_ref]), cmap='gray')\n",
    "plt.show()\n",
    "    \n",
    "print('       x_current     |      x_reference')\n",
    "x_hat0 = np.load(f'{path}x_hat0.npz')['a'][0,0,:,:]\n",
    "x_hat1 = np.load(f'{path}x_hat1.npz')['a'][0,0,:,:]\n",
    "x_hat2 = np.load(f'{path}x_hat2.npz')['a'][0,0,:,:]\n",
    "x_hat3 = np.load(f'{path}x_hat3.npz')['a'][0,0,:,:]\n",
    "x_hat4 = np.load(f'{path}x_hat4.npz')['a'][0,0,:,:]\n",
    "\n",
    "plt.imshow(np.hstack([x_hat0,x_hat1,x_hat2,x_hat3,x_hat4]), cmap='gray')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:  R1-eps|dim_128|z_dim_12|L_2|lambda_gp_10|bs_64|dpenalty_0|lambdaP_0|lambdaPM_0|lambdaMSE_1\n",
      "[-1.0, 1.0]\n",
      "[-1.0, 1.0]\n",
      "Finished Loading MNIST!\n",
      "0/937\n",
      "0.3628246784210205\n",
      "0.3306635022163391\n",
      "0.3028711974620819\n",
      "0.2787083387374878\n",
      "0.25379347801208496\n",
      "0.23185710608959198\n",
      "0.21509625017642975\n",
      "0.2020493596792221\n",
      "0.1895059049129486\n",
      "0.1736084520816803\n",
      "0.16808314621448517\n",
      "0.1601289063692093\n",
      "0.147793710231781\n",
      "0.14345335960388184\n",
      "0.13896317780017853\n",
      "0.13429290056228638\n",
      "0.13191775977611542\n",
      "0.12874507904052734\n",
      "0.12076707184314728\n",
      "0.11887286603450775\n",
      "0.11542440205812454\n",
      "0.11378762125968933\n",
      "0.11296244710683823\n",
      "0.11097872257232666\n",
      "0.10412988066673279\n",
      "0.10338439047336578\n",
      "0.1023823618888855\n",
      "0.10136085003614426\n",
      "0.09903062880039215\n",
      "0.09776449203491211\n",
      "0.0970434844493866\n",
      "0.09696976840496063\n",
      "0.09654946625232697\n",
      "0.08996689319610596\n",
      "0.08959425985813141\n",
      "0.09123285114765167\n",
      "0.08625246584415436\n",
      "0.0893346518278122\n",
      "0.08828805387020111\n",
      "0.08629836142063141\n",
      "0.08365120738744736\n",
      "0.08447464555501938\n",
      "0.08397144824266434\n",
      "0.08135458827018738\n",
      "0.08156908303499222\n",
      "0.08011890947818756\n",
      "0.07959981262683868\n",
      "0.0777798742055893\n",
      "0.08020388334989548\n",
      "0.07748939096927643\n",
      "0.07684797048568726\n",
      "0.07557283341884613\n",
      "0.07672110199928284\n",
      "0.07475100457668304\n",
      "0.07415752857923508\n",
      "0.07533610612154007\n",
      "0.07237137854099274\n",
      "0.06965857744216919\n",
      "0.07293583452701569\n",
      "0.06920601427555084\n",
      "0.07000822573900223\n",
      "0.06921223551034927\n",
      "0.06968237459659576\n",
      "0.06974643468856812\n",
      "0.06618468463420868\n",
      "0.0683012455701828\n",
      "0.06816358864307404\n",
      "0.06724273413419724\n",
      "0.06490510702133179\n",
      "0.06403465569019318\n",
      "0.06577038764953613\n",
      "0.06334289908409119\n",
      "0.06484924256801605\n",
      "0.062431834638118744\n",
      "0.06156977638602257\n",
      "0.061395060271024704\n",
      "0.06456152349710464\n",
      "0.06525759398937225\n",
      "0.06124098598957062\n",
      "0.0626755952835083\n",
      "0.06402206420898438\n",
      "0.061320140957832336\n",
      "0.05970853194594383\n",
      "0.0613667257130146\n",
      "0.05669955909252167\n",
      "0.059070199728012085\n",
      "0.06011329963803291\n",
      "0.058833494782447815\n",
      "0.0589432567358017\n",
      "0.05999310314655304\n",
      "0.05888818949460983\n",
      "0.06059335172176361\n",
      "0.05657918006181717\n",
      "0.05738792568445206\n",
      "0.055369917303323746\n",
      "0.05715256184339523\n",
      "0.05753414332866669\n",
      "0.05339546501636505\n",
      "0.0544615238904953\n",
      "0.05502026528120041\n",
      "100/937\n",
      "0.05288983881473541\n",
      "0.05504511296749115\n",
      "0.05787024274468422\n",
      "0.05566218122839928\n",
      "0.05424794182181358\n",
      "0.05182886868715286\n",
      "0.05218793451786041\n",
      "0.052988048642873764\n",
      "0.052576422691345215\n",
      "0.05377311259508133\n",
      "0.052109237760305405\n",
      "0.0540769100189209\n",
      "0.052066247910261154\n",
      "0.051204100251197815\n",
      "0.05234738439321518\n",
      "0.04952945187687874\n",
      "0.053823236376047134\n",
      "0.05119839310646057\n",
      "0.04922252893447876\n",
      "0.050670914351940155\n",
      "0.052332907915115356\n",
      "0.049546513706445694\n",
      "0.049654215574264526\n",
      "0.05167267099022865\n",
      "0.05362164229154587\n",
      "0.050245046615600586\n",
      "0.04941309988498688\n",
      "0.05004096031188965\n",
      "0.04923868924379349\n",
      "0.05122166872024536\n",
      "0.049974385648965836\n",
      "0.052556708455085754\n",
      "0.046524688601493835\n",
      "0.047975804656744\n",
      "0.048295967280864716\n",
      "0.04889300465583801\n",
      "0.04906267672777176\n",
      "0.047079816460609436\n",
      "0.047610267996788025\n",
      "0.05080484598875046\n",
      "0.048297345638275146\n",
      "0.04701904207468033\n",
      "0.045890867710113525\n",
      "0.04562399536371231\n",
      "0.04677063226699829\n",
      "0.04744946211576462\n",
      "0.046585701406002045\n",
      "0.048475880175828934\n",
      "0.04452717304229736\n",
      "0.045952118933200836\n",
      "0.04532451555132866\n",
      "0.04653548449277878\n",
      "0.0460311695933342\n",
      "0.044738370925188065\n",
      "0.04349946975708008\n",
      "0.046454064548015594\n",
      "0.044633787125349045\n",
      "0.04191303253173828\n",
      "0.044332846999168396\n",
      "0.04503120854496956\n",
      "0.042320020496845245\n",
      "0.04334605485200882\n",
      "0.04364409297704697\n",
      "0.04278350993990898\n",
      "0.040760450065135956\n",
      "0.04291178658604622\n",
      "0.04680261015892029\n",
      "0.043016355484724045\n",
      "0.04526258632540703\n",
      "0.04069123417139053\n",
      "0.04150918498635292\n",
      "0.04263260215520859\n",
      "0.0420428067445755\n",
      "0.04191845282912254\n",
      "0.042215242981910706\n",
      "0.04093599319458008\n",
      "0.04450054466724396\n",
      "0.0422067791223526\n",
      "0.04250151291489601\n",
      "0.04271259903907776\n",
      "0.04326552152633667\n",
      "0.040850888937711716\n",
      "0.04316946864128113\n",
      "0.039941977709531784\n",
      "0.04059720039367676\n",
      "0.04023215174674988\n",
      "0.041263945400714874\n",
      "0.04115802422165871\n",
      "0.041010547429323196\n",
      "0.040081195533275604\n",
      "0.042146697640419006\n",
      "0.04061009734869003\n",
      "0.04476238042116165\n",
      "0.04089687764644623\n",
      "0.041634462773799896\n",
      "0.04160073399543762\n",
      "0.03929675370454788\n",
      "0.040947169065475464\n",
      "0.04005523771047592\n",
      "0.039575450122356415\n",
      "200/937\n",
      "0.04249640926718712\n",
      "0.042102426290512085\n",
      "0.038299061357975006\n",
      "0.039409294724464417\n",
      "0.03792058676481247\n",
      "0.03815968334674835\n",
      "0.04145311564207077\n",
      "0.0393722727894783\n",
      "0.03962063789367676\n",
      "0.042896002531051636\n",
      "0.0444931834936142\n",
      "0.036900971084833145\n",
      "0.03864709660410881\n",
      "0.03856877610087395\n",
      "0.03834232687950134\n",
      "0.03840629383921623\n",
      "0.03700115904211998\n",
      "0.03748553246259689\n",
      "0.03969934582710266\n",
      "0.0388183519244194\n",
      "0.03765774145722389\n",
      "0.03701137378811836\n",
      "0.03590105473995209\n",
      "0.03881515935063362\n",
      "0.03693409264087677\n",
      "0.03958018496632576\n",
      "0.03957967460155487\n",
      "0.038219645619392395\n",
      "0.03715156391263008\n",
      "0.03809085488319397\n",
      "0.03561297059059143\n",
      "0.03800015151500702\n",
      "0.03519955277442932\n",
      "0.040447235107421875\n",
      "0.03841724991798401\n",
      "0.0382436066865921\n",
      "0.03977289795875549\n",
      "0.036088697612285614\n",
      "0.03763002157211304\n",
      "0.03636455908417702\n",
      "0.03560154139995575\n",
      "0.03476332873106003\n",
      "0.03688029199838638\n",
      "0.03393566980957985\n",
      "0.03476516529917717\n",
      "0.0334056094288826\n",
      "0.03635638952255249\n",
      "0.038748037070035934\n",
      "0.03517140448093414\n",
      "0.03559160232543945\n",
      "0.03715267777442932\n",
      "0.03700605779886246\n",
      "0.03508579730987549\n",
      "0.04064858704805374\n",
      "0.03597187250852585\n",
      "0.03483950346708298\n",
      "0.03314667195081711\n",
      "0.035570643842220306\n",
      "0.035144686698913574\n",
      "0.03429039567708969\n",
      "0.0379253625869751\n",
      "0.03705846518278122\n",
      "0.03529798611998558\n",
      "0.036556676030159\n",
      "0.03374822437763214\n",
      "0.035562749952077866\n",
      "0.03567612171173096\n",
      "0.03728073462843895\n",
      "0.03377600014209747\n",
      "0.03579677268862724\n",
      "0.035748496651649475\n",
      "0.032643698155879974\n",
      "0.0389290526509285\n",
      "0.034993499517440796\n",
      "0.0352591909468174\n",
      "0.03506551682949066\n",
      "0.035611435770988464\n",
      "0.03363797441124916\n",
      "0.033640261739492416\n",
      "0.034657225012779236\n",
      "0.03480907902121544\n",
      "0.033123426139354706\n",
      "0.03300086781382561\n",
      "0.03322707116603851\n",
      "0.03439725190401077\n",
      "0.03359328955411911\n",
      "0.034615010023117065\n",
      "0.03436337038874626\n",
      "0.0330374613404274\n",
      "0.03506608307361603\n",
      "0.033670343458652496\n",
      "0.03259238973259926\n",
      "0.03129838407039642\n",
      "0.03233013302087784\n",
      "0.033482205122709274\n",
      "0.03228676691651344\n",
      "0.03094533085823059\n",
      "0.033035267144441605\n",
      "0.03284907341003418\n",
      "0.030870236456394196\n",
      "300/937\n",
      "0.03347057104110718\n",
      "0.03334459289908409\n",
      "0.03324288874864578\n",
      "0.032032862305641174\n",
      "0.03474967181682587\n",
      "0.03390120714902878\n",
      "0.03708326071500778\n",
      "0.030984332785010338\n",
      "0.031215226277709007\n",
      "0.02932742051780224\n",
      "0.03131697326898575\n",
      "0.03243787959218025\n",
      "0.030053652822971344\n",
      "0.032530855387449265\n",
      "0.031705912202596664\n",
      "0.031174976378679276\n",
      "0.031991712749004364\n",
      "0.028550634160637856\n",
      "0.03145058453083038\n",
      "0.02979627624154091\n",
      "0.0320134311914444\n",
      "0.03176325559616089\n",
      "0.03053228370845318\n",
      "0.03218316659331322\n",
      "0.03083750233054161\n",
      "0.03073950856924057\n",
      "0.03438838943839073\n",
      "0.03157089278101921\n",
      "0.030103281140327454\n",
      "0.030971549451351166\n",
      "0.0322556234896183\n",
      "0.030870523303747177\n",
      "0.03098822757601738\n",
      "0.030710697174072266\n",
      "0.02848392352461815\n",
      "0.031077884137630463\n",
      "0.0298292338848114\n",
      "0.029116686433553696\n",
      "0.029470302164554596\n",
      "0.030643772333860397\n",
      "0.0291297510266304\n",
      "0.03173111006617546\n",
      "0.029042918235063553\n",
      "0.02919085882604122\n",
      "0.03234831988811493\n",
      "0.028438035398721695\n",
      "0.03190223127603531\n",
      "0.03029661439359188\n",
      "0.03132685273885727\n",
      "0.029469847679138184\n",
      "0.03100333735346794\n",
      "0.028853219002485275\n",
      "0.031435899436473846\n",
      "0.030560046434402466\n",
      "0.029684998095035553\n",
      "0.029363974928855896\n",
      "0.02880643680691719\n",
      "0.029516169801354408\n",
      "0.028656579554080963\n",
      "0.02794492617249489\n",
      "0.02854376658797264\n",
      "0.028999637812376022\n",
      "0.02888643369078636\n",
      "0.029966318979859352\n",
      "0.027956653386354446\n",
      "0.02991059049963951\n",
      "0.030480818822979927\n",
      "0.02862677350640297\n",
      "0.028748195618391037\n",
      "0.02729767933487892\n",
      "0.02813795581459999\n",
      "0.029323924332857132\n",
      "0.029392411932349205\n",
      "0.02909800037741661\n",
      "0.028759023174643517\n",
      "0.03014419600367546\n",
      "0.02805885672569275\n",
      "0.02853400446474552\n",
      "0.027901455760002136\n",
      "0.028184283524751663\n",
      "0.033323757350444794\n",
      "0.029641378670930862\n",
      "0.0274568609893322\n",
      "0.026106223464012146\n",
      "0.027413293719291687\n",
      "0.0285774115473032\n",
      "0.027809681370854378\n",
      "0.028524475172162056\n",
      "0.028119362890720367\n",
      "0.028218643739819527\n",
      "0.028657149523496628\n",
      "0.029407354071736336\n",
      "0.028114765882492065\n",
      "0.028807468712329865\n",
      "0.027316715568304062\n",
      "0.026106085628271103\n",
      "0.027193725109100342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026836421340703964\n",
      "0.026169031858444214\n",
      "0.026678217574954033\n",
      "400/937\n",
      "0.027211744338274002\n",
      "0.026904918253421783\n",
      "0.025560526177287102\n",
      "0.025374090299010277\n",
      "0.029421648010611534\n",
      "0.02665320411324501\n",
      "0.02948063053190708\n",
      "0.029659977182745934\n",
      "0.02870335802435875\n",
      "0.0311733428388834\n",
      "0.026658646762371063\n",
      "0.027410294860601425\n",
      "0.0268804132938385\n",
      "0.027852550148963928\n",
      "0.028362566605210304\n",
      "0.02654118835926056\n",
      "0.026936616748571396\n",
      "0.025315966457128525\n",
      "0.027774963527917862\n",
      "0.029610445722937584\n",
      "0.026076510548591614\n",
      "0.02504434436559677\n",
      "0.026540838181972504\n",
      "0.027279306203126907\n",
      "0.026554161682724953\n",
      "0.028018277138471603\n",
      "0.025721468031406403\n",
      "0.025497201830148697\n",
      "0.025767076760530472\n",
      "0.025874223560094833\n",
      "0.02420632727444172\n",
      "0.026979930698871613\n",
      "0.025856563821434975\n",
      "0.025764882564544678\n",
      "0.02485721930861473\n",
      "0.026595469564199448\n",
      "0.02519848197698593\n",
      "0.024398639798164368\n",
      "0.02454891800880432\n",
      "0.027883239090442657\n",
      "0.024264805018901825\n",
      "0.025694530457258224\n",
      "0.02512742206454277\n",
      "0.02438012883067131\n",
      "0.02476823329925537\n",
      "0.026295457035303116\n",
      "0.02558499574661255\n",
      "0.02768677845597267\n",
      "0.026595141738653183\n",
      "0.026693500578403473\n",
      "0.025383207947015762\n",
      "0.024599628522992134\n",
      "0.02490532398223877\n",
      "0.024522222578525543\n",
      "0.024855367839336395\n",
      "0.024358460679650307\n",
      "0.02348453551530838\n",
      "0.02541755884885788\n",
      "0.02466224506497383\n",
      "0.024381956085562706\n",
      "0.02408434823155403\n",
      "0.024459559470415115\n",
      "0.02746625430881977\n",
      "0.025649916380643845\n",
      "0.02619083970785141\n",
      "0.024644579738378525\n",
      "0.023890458047389984\n",
      "0.025343749672174454\n",
      "0.02462807483971119\n",
      "0.024266988039016724\n",
      "0.024640163406729698\n",
      "0.023749683052301407\n",
      "0.022965608164668083\n",
      "0.026005946099758148\n",
      "0.026516709476709366\n",
      "0.024619417265057564\n",
      "0.026207661256194115\n",
      "0.02439352497458458\n",
      "0.024110428988933563\n",
      "0.023492921143770218\n",
      "0.023738443851470947\n",
      "0.026705851778388023\n",
      "0.02654930017888546\n",
      "0.024239104241132736\n",
      "0.024974502623081207\n",
      "0.02440938726067543\n",
      "0.02630552276968956\n",
      "0.022936401888728142\n",
      "0.02465214766561985\n",
      "0.02605215460062027\n",
      "0.024121573194861412\n",
      "0.02319018356502056\n",
      "0.02401571162045002\n",
      "0.02376631833612919\n",
      "0.023603498935699463\n",
      "0.024957064539194107\n",
      "0.023908209055662155\n",
      "0.02331286109983921\n",
      "0.02420165203511715\n",
      "0.022907305508852005\n",
      "500/937\n",
      "0.02615726739168167\n",
      "0.02435321919620037\n",
      "0.024587620049715042\n",
      "0.0232606939971447\n",
      "0.025538712739944458\n",
      "0.02404811792075634\n",
      "0.025773730129003525\n",
      "0.022556710988283157\n",
      "0.027529705315828323\n",
      "0.02264898642897606\n",
      "0.023410819470882416\n",
      "0.02329173870384693\n",
      "0.024377424269914627\n",
      "0.021326690912246704\n",
      "0.024844955652952194\n",
      "0.04746794328093529\n",
      "0.025777500122785568\n",
      "0.027506306767463684\n",
      "0.024698898196220398\n",
      "0.024691592901945114\n",
      "0.023879867047071457\n",
      "0.025096876546740532\n",
      "0.02330831252038479\n",
      "0.023129936307668686\n",
      "0.02279731072485447\n",
      "0.024183403700590134\n",
      "0.02314867451786995\n",
      "0.02435152977705002\n",
      "0.023860525339841843\n",
      "0.024979688227176666\n",
      "0.02742341160774231\n",
      "0.024666130542755127\n",
      "0.024280928075313568\n",
      "0.024082643911242485\n",
      "0.022850926965475082\n",
      "0.023771408945322037\n",
      "0.02328372374176979\n",
      "0.023152774199843407\n",
      "0.02457897737622261\n",
      "0.024550890550017357\n",
      "0.024185091257095337\n",
      "0.02462119236588478\n",
      "0.022464793175458908\n",
      "0.023516330868005753\n",
      "0.023218967020511627\n",
      "0.0223369300365448\n",
      "0.02324933558702469\n",
      "0.0239645317196846\n",
      "0.02261090651154518\n",
      "0.023144608363509178\n",
      "0.022702928632497787\n",
      "0.02200901508331299\n",
      "0.023436833173036575\n",
      "0.023696813732385635\n",
      "0.023096878081560135\n",
      "0.022514022886753082\n",
      "0.022140778601169586\n",
      "0.021991576999425888\n",
      "0.02399754524230957\n",
      "0.02230781503021717\n",
      "0.023241613060235977\n",
      "0.023828376084566116\n",
      "0.02249469980597496\n",
      "0.02356850914657116\n",
      "0.024834588170051575\n",
      "0.022078869864344597\n",
      "0.022206556051969528\n",
      "0.021692438051104546\n",
      "0.02212044596672058\n",
      "0.02228621207177639\n",
      "0.02213326096534729\n",
      "0.02218383364379406\n",
      "0.022721873596310616\n",
      "0.026012282818555832\n",
      "0.02285102754831314\n",
      "0.02298090234398842\n",
      "0.022770296782255173\n",
      "0.022241957485675812\n",
      "0.02325000986456871\n",
      "0.023138057440519333\n",
      "0.021394487470388412\n",
      "0.023128049448132515\n",
      "0.021819986402988434\n",
      "0.02272290550172329\n",
      "0.020726284012198448\n",
      "0.0217595137655735\n",
      "0.022422047331929207\n",
      "0.02375241369009018\n",
      "0.021986842155456543\n",
      "0.021952195093035698\n",
      "0.023332705721259117\n",
      "0.02188432589173317\n",
      "0.021046463400125504\n",
      "0.022140998393297195\n",
      "0.022087767720222473\n",
      "0.021395981311798096\n",
      "0.022503653541207314\n",
      "0.023697245866060257\n",
      "0.020997945219278336\n",
      "0.02004861831665039\n",
      "600/937\n",
      "0.02206065133213997\n",
      "0.021361306309700012\n",
      "0.022327881306409836\n",
      "0.022439688444137573\n",
      "0.02166355773806572\n",
      "0.022143159061670303\n",
      "0.02195053920149803\n",
      "0.022200889885425568\n",
      "0.02053723856806755\n",
      "0.0220840722322464\n",
      "0.02178022265434265\n",
      "0.02192031964659691\n",
      "0.022154957056045532\n",
      "0.02248474769294262\n",
      "0.021330971270799637\n",
      "0.021980032324790955\n",
      "0.021707411855459213\n",
      "0.02099255472421646\n",
      "0.024230053648352623\n",
      "0.021674927324056625\n",
      "0.021835453808307648\n",
      "0.02141593024134636\n",
      "0.02361227571964264\n",
      "0.021242227405309677\n",
      "0.022152971476316452\n",
      "0.021459637209773064\n",
      "0.021183080971240997\n",
      "0.021347034722566605\n",
      "0.02222832664847374\n",
      "0.020267527550458908\n",
      "0.022650595754384995\n",
      "0.023634295910596848\n",
      "0.021529922261834145\n",
      "0.020977187901735306\n",
      "0.021779686212539673\n",
      "0.021132346242666245\n",
      "0.022281210869550705\n",
      "0.0208408385515213\n",
      "0.021746959537267685\n",
      "0.020888706669211388\n",
      "0.021157249808311462\n",
      "0.02421841025352478\n",
      "0.020942524075508118\n",
      "0.020348619669675827\n",
      "0.020230211317539215\n",
      "0.01935669407248497\n",
      "0.021405061706900597\n",
      "0.020269595086574554\n",
      "0.020448150113224983\n",
      "0.022880174219608307\n",
      "0.021215371787548065\n",
      "0.021048299968242645\n",
      "0.02080821432173252\n",
      "0.022324275225400925\n",
      "0.022359933704137802\n",
      "0.02078651636838913\n",
      "0.02040374092757702\n",
      "0.021048063412308693\n",
      "0.02049707993865013\n",
      "0.020636137574911118\n",
      "0.021166644990444183\n",
      "0.022233808413147926\n",
      "0.021392539143562317\n",
      "0.021129099652171135\n",
      "0.021274272352457047\n",
      "0.021528804674744606\n",
      "0.02090705744922161\n",
      "0.020614560693502426\n",
      "0.02042442373931408\n",
      "0.020962022244930267\n",
      "0.021231453865766525\n",
      "0.019259080290794373\n",
      "0.020847279578447342\n",
      "0.020404081791639328\n",
      "0.021186523139476776\n",
      "0.01955459639430046\n",
      "0.022536931559443474\n",
      "0.01974940486252308\n",
      "0.019956521689891815\n",
      "0.018945060670375824\n",
      "0.019933944568037987\n",
      "0.02125631272792816\n",
      "0.02124936878681183\n",
      "0.020611124113202095\n",
      "0.020111622288823128\n",
      "0.019292360171675682\n",
      "0.022018125280737877\n",
      "0.022417446598410606\n",
      "0.020519156008958817\n",
      "0.021815966814756393\n",
      "0.02082746848464012\n",
      "0.020796088501811028\n",
      "0.019992154091596603\n",
      "0.020344577729701996\n",
      "0.019918501377105713\n",
      "0.019864458590745926\n",
      "0.02034306898713112\n",
      "0.019046124070882797\n",
      "0.018935898318886757\n",
      "0.021040309220552444\n",
      "700/937\n",
      "0.020676961168646812\n",
      "0.020607631653547287\n",
      "0.020243389531970024\n",
      "0.01819300465285778\n",
      "0.019526833668351173\n",
      "0.019713839516043663\n",
      "0.021575557067990303\n",
      "0.020708782598376274\n",
      "0.020173387601971626\n",
      "0.019898435100913048\n",
      "0.02183876559138298\n",
      "0.020606456324458122\n",
      "0.019500166177749634\n",
      "0.02039068192243576\n",
      "0.039048317819833755\n",
      "0.021359767764806747\n",
      "0.0204925499856472\n",
      "0.020811529830098152\n",
      "0.020723318681120872\n",
      "0.02101174183189869\n",
      "0.02065315656363964\n",
      "0.020387519150972366\n",
      "0.019855109974741936\n",
      "0.01959056220948696\n",
      "0.0200115405023098\n",
      "0.021712714806199074\n",
      "0.019532203674316406\n",
      "0.02003191038966179\n",
      "0.019584447145462036\n",
      "0.02125631645321846\n",
      "0.019649852067232132\n",
      "0.01980363018810749\n",
      "0.020446553826332092\n",
      "0.01996557228267193\n",
      "0.020897869020700455\n",
      "0.018942106515169144\n",
      "0.019703296944499016\n",
      "0.01862163469195366\n",
      "0.019987385720014572\n",
      "0.020595721900463104\n",
      "0.019337283447384834\n",
      "0.020005837082862854\n",
      "0.0191543810069561\n",
      "0.018280673772096634\n",
      "0.02079669199883938\n",
      "0.021473072469234467\n",
      "0.01956244185566902\n",
      "0.02012457139790058\n",
      "0.01919063925743103\n",
      "0.021732209250330925\n",
      "0.020508065819740295\n",
      "0.0192791149020195\n",
      "0.019465332850813866\n",
      "0.019192436710000038\n",
      "0.019882656633853912\n",
      "0.01959303393959999\n",
      "0.020170610398054123\n",
      "0.020892178639769554\n",
      "0.01910734549164772\n",
      "0.019730214029550552\n",
      "0.020234759896993637\n",
      "0.02022366039454937\n",
      "0.019335579127073288\n",
      "0.019640790298581123\n",
      "0.019478268921375275\n",
      "0.019657697528600693\n",
      "0.020244035869836807\n",
      "0.019735969603061676\n",
      "0.019682627171278\n",
      "0.018868552520871162\n",
      "0.01974320597946644\n",
      "0.018397744745016098\n",
      "0.020010579377412796\n",
      "0.01970190554857254\n",
      "0.020042916759848595\n",
      "0.01938033103942871\n",
      "0.0196772962808609\n",
      "0.01979619637131691\n",
      "0.019685763865709305\n",
      "0.01881006360054016\n",
      "0.019224099814891815\n",
      "0.018644776195287704\n",
      "0.019305039197206497\n",
      "0.019947906956076622\n",
      "0.019059564918279648\n",
      "0.018481992185115814\n",
      "0.018457654863595963\n",
      "0.04603873938322067\n",
      "0.019765909761190414\n",
      "0.018795566633343697\n",
      "0.018896950408816338\n",
      "0.021822188049554825\n",
      "0.019685612991452217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022382300347089767\n",
      "0.020228412002325058\n",
      "0.020083675161004066\n",
      "0.01964884251356125\n",
      "0.020778445526957512\n",
      "0.020491912961006165\n",
      "0.01888551190495491\n",
      "800/937\n",
      "0.018959512934088707\n",
      "0.019330281764268875\n",
      "0.020433269441127777\n",
      "0.019716262817382812\n",
      "0.019529052078723907\n",
      "0.019014591351151466\n",
      "0.019239641726017\n",
      "0.019212232902646065\n",
      "0.01865638792514801\n",
      "0.018308395519852638\n",
      "0.018924513831734657\n",
      "0.018926016986370087\n",
      "0.020031500607728958\n",
      "0.01786578819155693\n",
      "0.018999677151441574\n",
      "0.019922368228435516\n",
      "0.019127853214740753\n",
      "0.01930018700659275\n",
      "0.020420214161276817\n",
      "0.01887098141014576\n",
      "0.01951136253774166\n",
      "0.018953019753098488\n",
      "0.018635660409927368\n",
      "0.0182430949062109\n",
      "0.020736057311296463\n",
      "0.018779098987579346\n",
      "0.02015647105872631\n",
      "0.019855067133903503\n",
      "0.019472535699605942\n",
      "0.01865270920097828\n",
      "0.019166024401783943\n",
      "0.019008982926607132\n",
      "0.018074415624141693\n",
      "0.018169179558753967\n",
      "0.01861572265625\n",
      "0.018785756081342697\n",
      "0.017614733427762985\n",
      "0.01850096881389618\n",
      "0.01998702436685562\n",
      "0.017994094640016556\n",
      "0.01792340725660324\n",
      "0.018621310591697693\n",
      "0.01903793215751648\n",
      "0.018151497468352318\n",
      "0.018841983750462532\n",
      "0.0186411514878273\n",
      "0.01824643649160862\n",
      "0.017992138862609863\n",
      "0.01953238807618618\n",
      "0.017517974600195885\n",
      "0.018402226269245148\n",
      "0.018868163228034973\n",
      "0.01839698664844036\n",
      "0.018216514959931374\n",
      "0.01896868646144867\n",
      "0.018142756074666977\n",
      "0.01771671324968338\n",
      "0.01809566095471382\n",
      "0.018342893570661545\n",
      "0.019889984279870987\n",
      "0.01961287297308445\n",
      "0.018548578023910522\n",
      "0.01896950975060463\n",
      "0.019419144839048386\n",
      "0.018074698746204376\n",
      "0.018616201356053352\n",
      "0.017738012596964836\n",
      "0.01893998309969902\n",
      "0.018397532403469086\n",
      "0.017766909673810005\n",
      "0.018106725066900253\n",
      "0.0188111811876297\n",
      "0.01828361675143242\n",
      "0.019624151289463043\n",
      "0.018667232245206833\n",
      "0.01845058985054493\n",
      "0.017464108765125275\n",
      "0.017914004623889923\n",
      "0.017609719187021255\n",
      "0.017381176352500916\n",
      "0.017161909490823746\n",
      "0.017949417233467102\n",
      "0.018338073045015335\n",
      "0.018702583387494087\n",
      "0.019258663058280945\n",
      "0.018546801060438156\n",
      "0.018176868557929993\n",
      "0.01880505681037903\n",
      "0.018754586577415466\n",
      "0.019600970670580864\n",
      "0.019440963864326477\n",
      "0.017939405515789986\n",
      "0.017193138599395752\n",
      "0.01813642308115959\n",
      "0.018194491043686867\n",
      "0.018235966563224792\n",
      "0.018033592030405998\n",
      "0.01812298223376274\n",
      "0.01808338053524494\n",
      "0.0174285639077425\n",
      "900/937\n",
      "0.018079746514558792\n",
      "0.017446551471948624\n",
      "0.01816311851143837\n",
      "0.017826272174715996\n",
      "0.017473723739385605\n",
      "0.018175771459937096\n",
      "0.018213436007499695\n",
      "0.017994023859500885\n",
      "0.01775301992893219\n",
      "0.01817353069782257\n",
      "0.01781683787703514\n",
      "0.017627863213419914\n",
      "0.017568543553352356\n",
      "0.01724124513566494\n",
      "0.017676329240202904\n",
      "0.018049275502562523\n",
      "0.01783733069896698\n",
      "0.01945103332400322\n",
      "0.018200471997261047\n",
      "0.01810877025127411\n",
      "0.018309107050299644\n",
      "0.017908191308379173\n",
      "0.01781511679291725\n",
      "0.017560390755534172\n",
      "0.01817380078136921\n",
      "0.01655663549900055\n",
      "0.018925512209534645\n",
      "0.01768963597714901\n",
      "0.018233004957437515\n",
      "0.017219047993421555\n",
      "0.018004709854722023\n",
      "0.018288742750883102\n",
      "0.01791801117360592\n",
      "0.027331868186593056\n",
      "0.017933422699570656\n",
      "0.017678409814834595\n",
      "0.017529290169477463\n"
     ]
    }
   ],
   "source": [
    "main_new(dim = 128,\n",
    "        z_dim = 12,\n",
    "        lambda_gp = 10,\n",
    "        bs = 64,\n",
    "        d_penalty = 0,\n",
    "        skip_fq = 1,\n",
    "        total_epochs = 3,\n",
    "        lambda_P = 0,\n",
    "        lambda_PN = 0,\n",
    "        lambda_MSE = 1,\n",
    "        L = 2,\n",
    "        path = './data/',\n",
    "        pre_path = 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dim = 128,\n",
    "        z_dim = 1,\n",
    "        lambda_gp = 10,\n",
    "        bs = 64,\n",
    "        d_penalty = 0,\n",
    "        skip_fq = 5,\n",
    "        total_epochs = 10,\n",
    "        lambda_P = 0,\n",
    "        lambda_PM = 0,\n",
    "        lambda_MSE = 0,\n",
    "        L = 2,\n",
    "        path = './data/',\n",
    "        pre_path = 'None'):\n",
    "        \n",
    "    #No quantization:\n",
    "    stochastic = True\n",
    "    quantize_latents = True\n",
    "    if L == -1:\n",
    "        stochastic = False\n",
    "        quantize_latents = False\n",
    "    print ('Stochastic: ', stochastic)\n",
    "    print ('Quantize: ', quantize_latents)\n",
    "    #Create folder:\n",
    "    folder_name='New_R1eps_dim_'+str(dim)+'|z_dim_'+str(z_dim)+'|L_'+str(L)+'|lambda_gp_'+str(lambda_gp) \\\n",
    "        +'|bs_'+str(bs)+'|dpenalty_'+str(d_penalty)+'|lambdaP_'+str(lambda_P)+'|lambdaPM_'+str(lambda_PM)+'|lambdaMSE_' + str(lambda_MSE)\n",
    "    print (\"Settings: \", folder_name)\n",
    "\n",
    "    os.makedirs('./saved_models/'+ folder_name, exist_ok=True)\n",
    "    f = open('./saved_models/'+ folder_name + \"/performance.txt\", \"a\")\n",
    "\n",
    "    #Define Models\n",
    "    discriminator = Discriminator_v3(out_ch=2) #Generator Side\n",
    "    discriminator_M = Discriminator_v3(out_ch=1) #Marginal Discriminator\n",
    "    ssf = ScaleSpaceFlow_R1eps(num_levels=1, dim=z_dim, stochastic=stochastic, quantize_latents=quantize_latents, L=L)\n",
    "\n",
    "    list_models = [discriminator, discriminator_M, ssf]\n",
    "\n",
    "    ssf.cuda()\n",
    "    discriminator.cuda()\n",
    "    discriminator_M.cuda()\n",
    "    \n",
    "\n",
    "    #Load models:\n",
    "    if pre_path != 'None':\n",
    "        ssf.motion_encoder.load_state_dict(torch.load(pre_path+'/m_enc.pth'))\n",
    "        ssf.motion_decoder.load_state_dict(torch.load(pre_path+'/m_dec.pth'))\n",
    "        ssf.P_encoder.load_state_dict(torch.load(pre_path+'/p_enc.pth'))\n",
    "        ssf.res_encoder.load_state_dict(torch.load(pre_path+'/r_enc.pth'))\n",
    "        ssf.res_decoder.load_state_dict(torch.load(pre_path+'/r_dec.pth'))\n",
    "        discriminator.load_state_dict(torch.load(pre_path+'/discriminator.pth'))\n",
    "        discriminator_M.load_state_dict(torch.load(pre_path+'/discriminator_M.pth'))\n",
    "    \n",
    "\n",
    "\n",
    "    #Define fixed model\n",
    "    I_dim = 12 #12 #8\n",
    "    I_L = 2\n",
    "    encoder = Encoder(dim=I_dim, nc=1, stochastic=True, quantize_latents=True, L=I_L) #Generator Side\n",
    "    decoder = Decoder_Iframe(dim=I_dim) #Generator Side\n",
    "    decoder_hat = Decoder_Iframe(dim=I_dim)\n",
    "\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    decoder_hat.cuda()\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    decoder_hat.eval()\n",
    "    encoder.load_state_dict(torch.load('./I3/I_frame_encoder_zdim_12_L_2.pth'))\n",
    "    decoder.load_state_dict(torch.load('./I3/I_frame_decoderMMSE_zdim_12_L_2.pth'))\n",
    "    decoder_hat.load_state_dict(torch.load('./I3/I_frame_decoder_zdim_12_L_2.pth'))\n",
    "\n",
    "    #Define Data Loader\n",
    "    train_loader, test_loader = get_dataloader(data_root=path, seq_len=8, batch_size=bs, num_digits=1)\n",
    "    loader_l = len(train_loader)\n",
    "    mse = torch.nn.MSELoss()\n",
    "\n",
    "    #discriminator.train()\n",
    "    opt_ssf= torch.optim.RMSprop(ssf.parameters(), lr=1e-5)\n",
    "    opt_d = torch.optim.RMSprop(discriminator.parameters(), lr=5e-5)\n",
    "    opt_dm = torch.optim.RMSprop(discriminator_M.parameters(), lr=5e-5)\n",
    "\n",
    "    list_opt = [opt_ssf, opt_d, opt_dm]\n",
    "\n",
    "    \n",
    "    for epoch in range(total_epochs):\n",
    "        set_models_state(list_models, 'train')\n",
    "        a1 = time.time()\n",
    "        for i,x in enumerate(train_loader):\n",
    "            print(f'{i}/{loader_l}')\n",
    "            #Set 0 gradient\n",
    "            set_opt_zero(list_opt)\n",
    "            \n",
    "            #Get the data\n",
    "            x = x.permute(0, 4, 1, 2, 3)\n",
    "            x = x.cuda().float()\n",
    "            x_cur = x[:,:,1,...]\n",
    "            with torch.no_grad():\n",
    "                hx = encoder(x[:,:,0,...])[0]\n",
    "                x_ref = decoder(hx).detach()\n",
    "                x_1_hat = decoder_hat(hx).detach()\n",
    "            #x_ref[x_ref < 0.1] = 0.0\n",
    "            x_hat = ssf(x_cur, x_ref, x_1_hat)\n",
    "\n",
    "\n",
    "\n",
    "            #Optimize discriminator\n",
    "            fake_vid = torch.cat((x_1_hat, x_hat), dim = 1)\n",
    "            real_vid = x[:,0,:2,...].detach() #this looks good!\n",
    "            fake_validity = discriminator(fake_vid.detach())\n",
    "            real_validity = discriminator(real_vid)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_vid.data, fake_vid.data)\n",
    "            errVD =  -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "            errVD.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "            #Optimize discriminator M\n",
    "            fake_img = x_hat.detach()\n",
    "            real_img = x[:,0,:1,...].detach()\n",
    "            fake_valid_m = discriminator_M(fake_img)\n",
    "            real_valid_m = discriminator_M(real_img)\n",
    "            gradient_penalty_m = compute_gradient_penalty(discriminator_M, fake_img.data, real_img.data)\n",
    "            errID =  -torch.mean(real_valid_m) + torch.mean(fake_valid_m) + lambda_gp * gradient_penalty_m\n",
    "            errID.backward()\n",
    "            opt_dm.step()\n",
    "            \n",
    "            \n",
    "            if i%skip_fq == 0:\n",
    "                x_cur = x_cur.detach()\n",
    "                x_ref = x_ref.detach()\n",
    "                x_1_hat = x_1_hat.detach()\n",
    "                x_hat = ssf(x_cur, x_ref, x_1_hat)\n",
    "\n",
    "                fake_vid = torch.cat((x_1_hat, x_hat), dim = 1)\n",
    "                fake_validity = discriminator(fake_vid)\n",
    "                errVG = -torch.mean(fake_validity)\n",
    "\n",
    "                fake_img = x_hat\n",
    "                fake_validity_im = discriminator_M(fake_img)\n",
    "                errIG = -torch.mean(fake_validity_im)\n",
    "\n",
    "                loss = lambda_MSE*mse(x_hat, x_cur) + lambda_P*errVG + lambda_PM*errIG\n",
    "                loss.backward()\n",
    "\n",
    "                opt_ssf.step()\n",
    "        a = time.time()\n",
    "        if epoch %10 == 0:\n",
    "            show_str= \"Epoch: \"+ str(epoch) + \"l_PM, l_P, l_MSE, d_penalty \" + str(lambda_PM) + str(lambda_P)+ \" \" \\\n",
    "            +str(lambda_MSE) + \" \" + str(d_penalty) + \" P loss: \" + str(cal_W1(ssf, encoder, decoder, decoder_hat, discriminator, discriminator_M, test_loader, list_models))\n",
    "            print (show_str)\n",
    "            f.write(show_str+\"\\n\")\n",
    "        b = time.time()\n",
    "        print('test time:', b-a)\n",
    "            \n",
    "        b1 = time.time()\n",
    "        print(f'epoch: {b1-a1}')\n",
    "    #show_str= \"Epoch: \"+ str(epoch) + \"l_PM, l_P, l_MSE, d_penalty \" + str(lambda_PM) + str(lambda_P)+ \" \" \\\n",
    "    #        +str(lambda_MSE) + \" \" + str(d_penalty) + \" P loss: \" + str(cal_W1(ssf, encoder, decoder, decoder_hat, discriminator, discriminator_M, test_loader, list_models))\n",
    "    #print (show_str)\n",
    "    #f.write(show_str+\"\\n\")\n",
    "\n",
    "\n",
    "    set_models_state(list_models, 'eval')\n",
    "    a = time.time()\n",
    "    torch.save(ssf.motion_encoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'm_enc.pth'))\n",
    "    torch.save(ssf.motion_decoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'm_dec.pth'))\n",
    "    torch.save(ssf.P_encoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'p_enc.pth'))\n",
    "    torch.save(ssf.res_encoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'r_enc.pth'))\n",
    "    torch.save(ssf.res_decoder.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'r_dec.pth' ))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'discriminator.pth'))\n",
    "    torch.save(discriminator_M.state_dict(), os.path.join(\"./saved_models/\" + folder_name, 'discriminator_M.pth'))\n",
    "    b = time.time()\n",
    "    print(f'save models: {b-a}')\n",
    "    f.close()\n",
    "\n",
    "    #save some figures\n",
    "    for i,x in enumerate(iter(train_loader)):\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        x = x.cuda().float()\n",
    "        break\n",
    "    np.savez_compressed(\"./saved_models/\" + folder_name+\"/x\", a=x.detach().cpu().numpy())\n",
    "\n",
    "    for i in range(5): #generate same figure 5 times\n",
    "        x_cur = x[:,:,1,...]\n",
    "        x_ref = x[:,:,0,...]\n",
    "        x_hat = ssf(x_cur, x_ref)\n",
    "        np.savez_compressed(\"./saved_models/\" + folder_name+\"/x_hat\"+str(i), a=x_hat.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
